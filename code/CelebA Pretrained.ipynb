{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BDbJtpbFA-Zx"
   },
   "source": [
    "# Note on training:\n",
    "\n",
    "Rather than data being the image and target being the correct value, the data will be three images and the target will be which image is more similar to the first image.\n",
    "\n",
    "Create a custom dataset with PyTorch using the 3 images as the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17151,
     "status": "ok",
     "timestamp": 1621574446374,
     "user": {
      "displayName": "Danny Saad",
      "photoUrl": "",
      "userId": "08768728463630714575"
     },
     "user_tz": 240
    },
    "id": "_RRfisl-96d1",
    "outputId": "b008fde2-48ac-4086-b93a-e72035f4399d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in /opt/conda/lib/python3.6/site-packages (0.10.24)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /opt/conda/lib/python3.6/site-packages (from wandb) (3.1.14)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.6/site-packages (from wandb) (2.22.0)\n",
      "Requirement already satisfied: configparser>=3.8.1 in /opt/conda/lib/python3.6/site-packages (from wandb) (5.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.6/site-packages (from wandb) (2.8.1)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.6/site-packages (from wandb) (5.3)\n",
      "Requirement already satisfied: sentry-sdk>=0.4.0 in /opt/conda/lib/python3.6/site-packages (from wandb) (1.0.0)\n",
      "Requirement already satisfied: Click>=7.0 in /opt/conda/lib/python3.6/site-packages (from wandb) (7.1.1)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in /opt/conda/lib/python3.6/site-packages (from wandb) (1.0.1)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.6/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: pathtools in /opt/conda/lib/python3.6/site-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.6/site-packages (from wandb) (5.6.7)\n",
      "Requirement already satisfied: promise<3,>=2.0 in /opt/conda/lib/python3.6/site-packages (from wandb) (2.3)\n",
      "Requirement already satisfied: subprocess32>=3.5.3 in /opt/conda/lib/python3.6/site-packages (from wandb) (3.5.4)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /opt/conda/lib/python3.6/site-packages (from wandb) (3.15.6)\n",
      "Requirement already satisfied: six>=1.13.0 in /opt/conda/lib/python3.6/site-packages (from wandb) (1.14.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.6/site-packages (from GitPython>=1.0.0->wandb) (4.0.7)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests<3,>=2.0.0->wandb) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests<3,>=2.0.0->wandb) (2019.11.28)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests<3,>=2.0.0->wandb) (1.25.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
      "Requirement already satisfied: smmap<5,>=3.0.1 in /opt/conda/lib/python3.6/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (4.0.0)\n"
     ]
    }
   ],
   "source": [
    "# installs\n",
    "!pip install wandb\n",
    "\n",
    "# Library imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import wandb\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms, utils\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchviz import make_dot\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import math\n",
    "%matplotlib inline\n",
    "\n",
    "from skimage import io, transform\n",
    "from sklearn.metrics import roc_auc_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lKZimeron_zu"
   },
   "source": [
    "# Meta-criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 526,
     "status": "ok",
     "timestamp": 1621574475836,
     "user": {
      "displayName": "Danny Saad",
      "photoUrl": "",
      "userId": "08768728463630714575"
     },
     "user_tz": 240
    },
    "id": "4heO4Uo7Owze"
   },
   "outputs": [],
   "source": [
    "def metacriteria_attractive(A_attr: dict, B_attr: dict, C_attr: dict):\n",
    "  A_attractive = A_attr['Attractive']\n",
    "  B_attractive = B_attr['Attractive']\n",
    "  C_attractive = C_attr['Attractive']\n",
    "\n",
    "  if A_attractive == B_attractive:\n",
    "    if A_attractive == C_attractive:\n",
    "      return 0.5\n",
    "    return 1\n",
    "  if A_attractive == C_attractive:\n",
    "    return 0\n",
    "  return 0.5\n",
    "\n",
    "def metacriteria_similar_hair(a, b, c):\n",
    "  return 0\n",
    "\n",
    "def metacriteria_similar_facial_hair(a, b, c):\n",
    "  if(a == b and a != c):\n",
    "    return 1\n",
    "  if(a == b and a == c):\n",
    "    return 0.5\n",
    "  return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7kpwVnA-oKgG"
   },
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "cellView": "both",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 747,
     "status": "ok",
     "timestamp": 1621574476541,
     "user": {
      "displayName": "Danny Saad",
      "photoUrl": "",
      "userId": "08768728463630714575"
     },
     "user_tz": 240
    },
    "id": "tsQfpCSWaEVQ",
    "outputId": "bb9e1f72-d402-492a-ae11-1ffe1271c900"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:3\n"
     ]
    }
   ],
   "source": [
    "# data settings:\n",
    "\n",
    "n_test = 5000\n",
    "# metacriteria = metacriteria_sameNumber\n",
    "batch_size=64 #input batch size for training (default: 64)\n",
    "batch_size_test=1000 \n",
    "\n",
    "\n",
    "# model settings\n",
    "epochs=40 #number of epochs to train (default: 14)\n",
    "lr=3 #learning rate (default: 1.0)\n",
    "gamma=0.7 #Learning rate step gamma (default: 0.7)\n",
    "seed=42 #random seed (default: 42)\n",
    "save_model=False #save the trained model (default: False)\n",
    "\n",
    "# misc settings\n",
    "no_cuda=False #disables CUDA training (default: True)\n",
    "use_cuda = not no_cuda and torch.cuda.is_available()\n",
    "torch.manual_seed(seed)\n",
    "device = torch.device(3)\n",
    "kwargs = {'num_workers': 10, 'pin_memory': True}\n",
    "\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oYQLa-J7Dl-d"
   },
   "source": [
    "### Data Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "executionInfo": {
     "elapsed": 2228,
     "status": "ok",
     "timestamp": 1621574478395,
     "user": {
      "displayName": "Danny Saad",
      "photoUrl": "",
      "userId": "08768728463630714575"
     },
     "user_tz": 240
    },
    "id": "qtWlI9ovDqtz"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.git',\n",
       " 'code',\n",
       " 'docker',\n",
       " '.ipynb_checkpoints',\n",
       " 'wandb',\n",
       " 'vae.pt',\n",
       " '.gitignore',\n",
       " 'data',\n",
       " 'models',\n",
       " 'README.md']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Danny's Working Directory\n",
    "data_dir = 'data/celeba'\n",
    "encoder_path = 'models/celeba_encoder_64.pt'\n",
    "\n",
    "# Danny's CelebA Paths\n",
    "os.chdir('/home')\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bREBMPnRuHsx"
   },
   "source": [
    "#CelebA Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OPi-o2-UelWP"
   },
   "source": [
    "### Get Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c-4_39uEuLxM"
   },
   "source": [
    "### Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "executionInfo": {
     "elapsed": 109626,
     "status": "ok",
     "timestamp": 1621574586980,
     "user": {
      "displayName": "Danny Saad",
      "photoUrl": "",
      "userId": "08768728463630714575"
     },
     "user_tz": 240
    },
    "id": "QpKeiYHTuPBc"
   },
   "outputs": [],
   "source": [
    "img_shape = (64, 64)\n",
    "img_channels_shape = (1,64,64)\n",
    "\n",
    "# transforms\n",
    "tfms_train = transforms.Compose([\n",
    "    transforms.Grayscale(),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.Resize(img_shape),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "tfms_val = tfms_test = transforms.Compose([\n",
    "    transforms.Grayscale(),\n",
    "    transforms.Resize(img_shape),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uR8uo8kIuSaz"
   },
   "source": [
    "###Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "executionInfo": {
     "elapsed": 108096,
     "status": "ok",
     "timestamp": 1621574586981,
     "user": {
      "displayName": "Danny Saad",
      "photoUrl": "",
      "userId": "08768728463630714575"
     },
     "user_tz": 240
    },
    "id": "tosmLkzRumPf"
   },
   "outputs": [],
   "source": [
    "class CelebA(Dataset):\n",
    "  def __init__(self, root_dir: str, partition: str, transform=None):\n",
    "    # the image data\n",
    "    self.img_dir = f\"{root_dir}/img_align_celeba\"\n",
    "\n",
    "    # the partition data\n",
    "    if partition=='train':\n",
    "      self.partition_ix = 0\n",
    "    elif partition=='val':\n",
    "      self.partition_ix = 1\n",
    "    elif partition=='test':\n",
    "      self.partition_ix = 2\n",
    "    else:\n",
    "      raise ValueError(\"partition must be one of 'train', 'val', or 'test'\")\n",
    "\n",
    "    df_partitions = pd.read_csv(f\"{root_dir}/list_eval_partition.csv\")\n",
    "    self.df_partitions = df_partitions[df_partitions['partition']==self.partition_ix]\n",
    "\n",
    "    # the attribute data\n",
    "    df_attributes = pd.read_csv(f\"{root_dir}/list_attr_celeba.csv\")\n",
    "\n",
    "    # filter the attributes\n",
    "    self.df_attributes = self.df_partitions.merge(df_attributes, how='left', on='image_id').drop(columns=['partition'])\n",
    "\n",
    "    # transforms\n",
    "    self.transform = transform\n",
    "\n",
    "  def __len__(self):\n",
    "    return self.df_attributes.shape[0]\n",
    "\n",
    "  def __getitem__(self, ix):\n",
    "    # image\n",
    "    filepath = f\"{self.img_dir}/{self.df_attributes.image_id[ix]}\"\n",
    "    image = Image.open(filepath)\n",
    "    if self.transform:\n",
    "      image = self.transform(image)\n",
    "\n",
    "    # attributes\n",
    "    attributes =self.df_attributes.iloc[ix, 1:].to_dict()\n",
    "\n",
    "    return {'image': image, 'attributes': attributes}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IYA_iyl2b5dt"
   },
   "source": [
    "#Triplet dataset class\n",
    "Where the inputs are:\n",
    "*   List of triple indices to use for training\n",
    "*   Original training set\n",
    "*   Function for evaluating meta-criteria\n",
    "\n",
    "And the output is:\n",
    "*   ((A: image,B: image,C: image), target: bool)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mRRR6V5ipXvv"
   },
   "source": [
    "### Defining the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "executionInfo": {
     "elapsed": 107290,
     "status": "ok",
     "timestamp": 1621574586981,
     "user": {
      "displayName": "Danny Saad",
      "photoUrl": "",
      "userId": "08768728463630714575"
     },
     "user_tz": 240
    },
    "id": "n36_Tzi5sHim"
   },
   "outputs": [],
   "source": [
    "class ImageTripletDataset(Dataset):\n",
    "    \"\"\"Dataset of triplets of images\"\"\"\n",
    "\n",
    "    def __init__(self, ImageDataset, num_triplets, criteria):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            ImageDataset (torch.utils.data.Dataset):  A pytorch dataset that serves individual images\n",
    "            num_triplets (int): Number of triplets\n",
    "        \"\"\"\n",
    "        self.imagedataset=ImageDataset\n",
    "        self.num_triplets = num_triplets\n",
    "        self.criteria = criteria\n",
    "\n",
    "        # generate indices list\n",
    "        self.indices = np.random.randint(0, len(self.imagedataset), (self.num_triplets, 3))\n",
    "        # TODO: make sure there are no duplicates?\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_triplets\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        Aix, Bix, Cix = self.indices[idx]\n",
    "\n",
    "        A = self.imagedataset[Aix]\n",
    "        B = self.imagedataset[Bix]\n",
    "        C = self.imagedataset[Cix]\n",
    "\n",
    "        A_img, A_attr = A['image'], A['attributes']\n",
    "        B_img, B_attr = B['image'], B['attributes']\n",
    "        C_img, C_attr = C['image'], C['attributes']\n",
    "\n",
    "        sample = {'A': A_img, 'B': B_img, 'C':C_img, \n",
    "                  'target': self.criteria(A_attr,B_attr,C_attr),\n",
    "                  'image_indices': (Aix, Bix, Cix), 'image_digits': (A_attr, B_attr, C_attr)}\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-I16_W26paIC"
   },
   "source": [
    "### Creating datasets/dataloaders for training and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F4XUJvI5St35"
   },
   "source": [
    "# VAE Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetBlock(nn.Module):\n",
    "    r\"\"\"Resnet style block for predicting color shift masks on input images. \n",
    "    \n",
    "    Args:\n",
    "        num_in (int) - number of input channels (and output channels)\n",
    "        num_features (int) - number of intermediate channels in resnet block\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_in, num_mid, kernel_size=5):\n",
    "\n",
    "        super(ResNetBlock, self).__init__()\n",
    "        \n",
    "        self.res = nn.Sequential(OrderedDict([\n",
    "            # conv block 1\n",
    "            ('conv0', nn.Conv2d(num_in, num_mid, kernel_size, stride=1,\n",
    "                                padding=(kernel_size-1)//2, bias=False)),\n",
    "            ('norm0', nn.BatchNorm2d(num_mid)),\n",
    "            ('relu0', nn.ReLU(inplace=True)),\n",
    "            # conv block 2\n",
    "            ('conv1', nn.Conv2d(num_mid, num_in, kernel_size, stride=1,\n",
    "                                padding=(kernel_size-1)//2, bias=False)),\n",
    "            ('norm1', nn.BatchNorm2d(num_in)),\n",
    "\n",
    "        ]))\n",
    "        \n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # resnet style output: add input to features at relu\n",
    "        return self.relu1(x + self.res(x))\n",
    "    \n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, n_in, n_mid, n_out, kernel_size=5):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        \n",
    "        self.block = nn.Sequential(\n",
    "            ResNetBlock(n_in, n_mid, kernel_size),\n",
    "            nn.Conv2d(n_in, n_out, kernel_size=kernel_size, padding=(kernel_size-1)//2, stride=2),\n",
    "            nn.BatchNorm2d(n_out),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "        \n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JdF5QzZfQp4c"
   },
   "source": [
    "### Encoder \n",
    "VAE Encoder copied from a separate VAE Training Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "executionInfo": {
     "elapsed": 105889,
     "status": "ok",
     "timestamp": 1621574586981,
     "user": {
      "displayName": "Danny Saad",
      "photoUrl": "",
      "userId": "08768728463630714575"
     },
     "user_tz": 240
    },
    "id": "ZpuDa2yWQ9rs"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_shape, n_mid=(96,64,32,32), n_res=64, kernel_size=5):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            # start\n",
    "            nn.Conv2d(input_shape[0], n_mid[0], kernel_size=7, padding=3, stride=2),\n",
    "            #nn.MaxPool2d(kernel_size=7, stride=2, padding=3),\n",
    "            nn.BatchNorm2d(n_mid[0]),\n",
    "            nn.ReLU(),\n",
    "            # encoding blocks\n",
    "            EncoderBlock(n_mid[0], n_res, n_mid[1], kernel_size),\n",
    "            EncoderBlock(n_mid[1], n_res, n_mid[2], kernel_size),\n",
    "            EncoderBlock(n_mid[2], n_res, n_mid[3], kernel_size),\n",
    "            Flatten()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TWbJHb3_ozKU"
   },
   "source": [
    "# Triplet model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6mzuWAqzqBiA"
   },
   "source": [
    "### Triplet probability class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "executionInfo": {
     "elapsed": 104460,
     "status": "ok",
     "timestamp": 1621574586982,
     "user": {
      "displayName": "Danny Saad",
      "photoUrl": "",
      "userId": "08768728463630714575"
     },
     "user_tz": 240
    },
    "id": "jBlDnltUp9r9"
   },
   "outputs": [],
   "source": [
    "# From Van der Maaten: https://ieeexplore.ieee.org/abstract/document/6349720/?casa_token=_dWfybjO5O4AAAAA:T4Al2g3ZWaHcRwCzlp9QqRji1amJ-uCHwbEyAOHk3_AH9uIGjs4iBFaUf8XO-wqLpd7D2BH1eQ\n",
    "# this has 0 free parameters\n",
    "class TripletProbability(nn.Module):\n",
    "  def __init__(self, alpha):\n",
    "    super(TripletProbability, self).__init__()\n",
    "    self.alpha=alpha\n",
    "\n",
    "  def t_dist(self, d):\n",
    "    return (1+d**2/self.alpha)**(-1*(self.alpha+1)/2)\n",
    "  \n",
    "  def forward(self, dAB, dAC):\n",
    "    tAB = self.t_dist(dAB)\n",
    "    tAC = self.t_dist(dAC)\n",
    "    return tAB / (tAB + tAC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "veC09SWTqTV5"
   },
   "source": [
    "### Pairwise-distance neural network module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "executionInfo": {
     "elapsed": 103633,
     "status": "ok",
     "timestamp": 1621574586982,
     "user": {
      "displayName": "Danny Saad",
      "photoUrl": "",
      "userId": "08768728463630714575"
     },
     "user_tz": 240
    },
    "id": "6GSBvQCGqccC"
   },
   "outputs": [],
   "source": [
    "class PairwiseDistance(nn.Module):\n",
    "  def __init__(self, n_hid):\n",
    "    super(PairwiseDistance, self).__init__()\n",
    "    self.n_hid = n_hid\n",
    "\n",
    "    self.f = nn.Sequential(\n",
    "        nn.Linear(2*self.n_hid, self.n_hid),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(self.n_hid,1)\n",
    "    )\n",
    "  \n",
    "  def forward(self, A, B):\n",
    "    # A: [batch_size, n_hid]\n",
    "    # B: [batch_size, n_hid]\n",
    "    ### Size of A and B are 64 each\n",
    "    print(\"A Size: \", A.shape)\n",
    "    AB = torch.cat([A, B], dim=0) # I changed this from 1 to 0 in order to make the mat dims match\n",
    "    print(\"AB Size: \", AB.shape)\n",
    "    ### Size of AB is 128\n",
    "    squeezed = self.f(AB).squeeze()\n",
    "    print(\"Squeezed AB Size: \", squeezed.shape)\n",
    "    return squeezed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AZuXiuebq1oW"
   },
   "source": [
    "### TripletNet \n",
    "A neural network for modeling whether or not a triplet of images, (A, B, C), \n",
    "satisfies the proposition \"A is more similar to B than C\" according to a meta-\n",
    "criteria. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "executionInfo": {
     "elapsed": 102588,
     "status": "ok",
     "timestamp": 1621574587234,
     "user": {
      "displayName": "Danny Saad",
      "photoUrl": "",
      "userId": "08768728463630714575"
     },
     "user_tz": 240
    },
    "id": "dZuR-N5Jrrm8"
   },
   "outputs": [],
   "source": [
    "class TripletNet(nn.Module):\n",
    "    def __init__(self, n_hid=10, alpha=1):\n",
    "        super(TripletNet, self).__init__()\n",
    "\n",
    "        self.n_hid=n_hid\n",
    "        self.alpha=alpha\n",
    "\n",
    "        # feature encoder\n",
    "        # resuse for each input image\n",
    "        self.encoder = Encoder(img_channels_shape, kernel_size=5)\n",
    "        \n",
    "        # distance computer: takes two samples and computes a distance\n",
    "        # reuse this for pairs (A, B) and (A, C)\n",
    "        # can make this more complex or more simple in future\n",
    "        self.pairwise_distance = PairwiseDistance(n_hid=self.n_hid)\n",
    "\n",
    "        # triplet probability computer defined in the class above\n",
    "        self.triplet_probability = TripletProbability(self.alpha)\n",
    "        \n",
    "    def forward(self, A, B, C):\n",
    "      # first compute all of the encodings\n",
    "        print(\"A shape pre encoder: \" , A.shape)\n",
    "        A, B, C = [self.encoder(x) for x in (A,B,C)]\n",
    "        print(\"A shape post encoder: \", A.shape)\n",
    "        \n",
    "      # then get the pairwise distances\n",
    "        ### Size of A B C are 64 each - correct\n",
    "        dAB = self.pairwise_distance(A, B)\n",
    "        dAC = self.pairwise_distance(A, C)\n",
    "        ### Size of dAB and dAC are 128 each - incorrect\n",
    "        \n",
    "      # finally return the triplet probability\n",
    "        return self.triplet_probability(dAB, dAC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y8et6eIMcY8E"
   },
   "source": [
    "# Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "executionInfo": {
     "elapsed": 101519,
     "status": "ok",
     "timestamp": 1621574587234,
     "user": {
      "displayName": "Danny Saad",
      "photoUrl": "",
      "userId": "08768728463630714575"
     },
     "user_tz": 240
    },
    "id": "RYKoSAKLJODw"
   },
   "outputs": [],
   "source": [
    "#@title Training and testing functions\n",
    "def train(model, criteria, device, loader, optimizer):\n",
    "  model.train()\n",
    "\n",
    "  mean_batch_losses = []\n",
    "  for batch_idx, batch_dict in enumerate(loader):\n",
    "    A, B, C, target = [batch_dict[key].to(device) for key in [\"A\", \"B\", \"C\", \"target\"]]\n",
    "    optimizer.zero_grad()\n",
    "    output = model(A,B,C)\n",
    "    ### output is computing 128 probabilities instead of 64\n",
    "    loss = criteria(output.float(), target.float())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    mean_batch_losses.append(loss.item())\n",
    "        \n",
    "  return np.mean(mean_batch_losses)\n",
    "\n",
    "            \n",
    "def test(model, criteria, device, loader):\n",
    "    model.eval()\n",
    "\n",
    "    mean_batch_losses = []\n",
    "    outputs = []\n",
    "    targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch_dict in enumerate(loader):\n",
    "          A, B, C, target = [batch_dict[key].to(device) for key in [\"A\", \"B\", \"C\", \"target\"]]\n",
    "          output = model(A, B, C)\n",
    "          loss = criteria(output.float(), target.float()) \n",
    "\n",
    "          # store results\n",
    "          mean_batch_losses.append(loss.item())\n",
    "          outputs.append(output)\n",
    "          targets.append(target)\n",
    "\n",
    "    outputs = torch.cat(outputs)\n",
    "    targets = torch.cat(targets)\n",
    "\n",
    "\n",
    "    \n",
    "    return np.mean(mean_batch_losses), outputs, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d5J1Rvt1-tyJ"
   },
   "source": [
    "# TRAINING LOOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eJWyPMC79S1V"
   },
   "source": [
    "### Loop Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "executionInfo": {
     "elapsed": 100106,
     "status": "ok",
     "timestamp": 1621574587478,
     "user": {
      "displayName": "Danny Saad",
      "photoUrl": "",
      "userId": "08768728463630714575"
     },
     "user_tz": 240
    },
    "id": "hJWMoHL19dl5"
   },
   "outputs": [],
   "source": [
    "def makeData(n_train, metacriteria):\n",
    "  #training data\n",
    " # image_dataset_train = dataPath\n",
    "    image_dataset_train = CelebA(data_dir, \"train\", transform = tfms_train)\n",
    "    triplet_dataset_train = ImageTripletDataset(image_dataset_train, n_train, metacriteria)\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "      triplet_dataset_train,\n",
    "      batch_size=batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "    # testing data\n",
    "    image_dataset_test = CelebA(data_dir, \"test\", transform = tfms_test)\n",
    "    triplet_dataset_test = ImageTripletDataset(image_dataset_test, n_test, metacriteria)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "      triplet_dataset_test,\n",
    "      batch_size=batch_size_test, shuffle=True, **kwargs)\n",
    "\n",
    "    return train_loader,test_loader\n",
    "\n",
    "\n",
    "\n",
    "def makeModel():\n",
    "  model = TripletNet(n_hid=256).to(device)\n",
    "  encoder_state_dict = torch.load(encoder_path)\n",
    "  model.encoder.load_state_dict(encoder_state_dict)\n",
    "  \n",
    "  return model\n",
    "\n",
    "\n",
    "\n",
    "def getLowestError(model, train_loader, test_loader):\n",
    "  optimizer = optim.Adadelta(model.parameters(), lr=lr)\n",
    "  scheduler = StepLR(optimizer, step_size=1, gamma=gamma)\n",
    "  criteria = nn.BCELoss()\n",
    "\n",
    "  best_test = 10**10\n",
    "  best_train = 10**10\n",
    "  best_auroc_filtered = 0.5\n",
    "  best_auroc_all = 0.5\n",
    "  best_accuracy_all = 0.5\n",
    "  best_accuracy_filtered = 0.5\n",
    "  best_epoch = 0\n",
    "  for epoch in range(1, epochs + 1):\n",
    "    train_loss = train(model, criteria, device, train_loader, optimizer)\n",
    "    test_loss, outputs, targets = test(model, criteria, device, test_loader)\n",
    "    scheduler.step()\n",
    "\n",
    "    auroc_filtered = getAUROC_filtered(outputs, targets)\n",
    "    auroc_all = getAUROC_all(outputs, targets)\n",
    "    accuracy_filtered = getAccuracy_filtered(outputs, targets)\n",
    "    accuracy_all = getAccuracy_all(outputs,targets)\n",
    "    \n",
    "    print(\"Train Loss: %0.3f. Test Loss: %0.3f. AUROC_Filtered: %0.3f. AUROC_All: %0.3f. Accuracy Filtered: %0.3f. Accuracy All: %0.3f. Epoch: %i\" % (train_loss, test_loss, auroc_filtered, auroc_all, accuracy_filtered, accuracy_all, epoch))\n",
    "    if test_loss<best_test:\n",
    "      best_test = test_loss\n",
    "      best_train = train_loss.item()\n",
    "      best_auroc_filtered = auroc_filtered\n",
    "      best_auroc_all = auroc_all\n",
    "      best_accuracy_filtered = accuracy_filtered\n",
    "      best_accuracy_all = accuracy_all\n",
    "      best_epoch = epoch\n",
    "    if epoch > best_epoch + 5:\n",
    "      break\n",
    "\n",
    "  #return best_test, best_auroc, best_epoch\n",
    "  return best_test, best_epoch, best_accuracy_all, best_accuracy_filtered, best_auroc_filtered, best_auroc_all\n",
    "\n",
    "def getAUROC_filtered(outputs, targets):\n",
    "  ix_keep = targets!=0.5\n",
    "  filteredOutputs = outputs[ix_keep]\n",
    "  filteredTargets = targets[ix_keep]\n",
    "  return roc_auc_score(filteredTargets.cpu().numpy(), filteredOutputs.cpu().numpy())\n",
    "\n",
    "def getAUROC_all(outputs, targets):\n",
    "  predictions = outputs.cpu().numpy()\n",
    "  targets = targets.cpu().numpy()\n",
    "  binned_predictions = []\n",
    "  binned_targets = []\n",
    "\n",
    "  bins = [0, 1/2, 1]\n",
    "  bin_indices = np.digitize(predictions, bins)\n",
    "  for index in bin_indices:\n",
    "    if index == 1:\n",
    "      binned_predictions.append(0)\n",
    "    elif index == 2:\n",
    "      binned_predictions.append(1)\n",
    "\n",
    "  bin_indices = np.digitize(targets, bins)\n",
    "  for target in targets:\n",
    "    if target == 0:\n",
    "      binned_targets.append(0)\n",
    "    elif target == 0.5:\n",
    "      binned_targets.append(1)\n",
    "    elif target == 1:\n",
    "      binned_targets.append(1)\n",
    "  \n",
    "  return roc_auc_score(binned_targets, binned_predictions)\n",
    "\n",
    "def getAccuracy_filtered(outputs, targets):\n",
    "  ix_keep = targets!=0.5\n",
    "  filteredOutputs = outputs[ix_keep]\n",
    "  filteredTargets = targets[ix_keep]\n",
    "  filteredOutputs = filteredOutputs.cpu().numpy()\n",
    "  filteredTargets = filteredTargets.cpu().numpy()\n",
    "\n",
    "  correct = 0;\n",
    "  results = np.equal(filteredOutputs, filteredTargets)\n",
    "  for result in results:\n",
    "    if result:\n",
    "      correct += 1\n",
    "  \n",
    "  return (correct / filteredTargets.size)\n",
    "\n",
    "\n",
    "def getAccuracy_all(predictions, targets):\n",
    "  predictions = predictions.cpu().numpy()\n",
    "  targets = targets.cpu().numpy()\n",
    "  new_predictions = []\n",
    "  correct = 0\n",
    "\n",
    "  bins = [0, 1/3, 2/3, 1]\n",
    "  bin_indices = np.digitize(predictions, bins)\n",
    "  for index in bin_indices:\n",
    "    if index == 1:\n",
    "      new_predictions.append(0)\n",
    "    elif index == 2:\n",
    "      new_predictions.append(0.5)\n",
    "    elif index == 3:\n",
    "      new_predictions.append(1)\n",
    "  \n",
    "  results = np.equal(new_predictions,targets)\n",
    "  for result in results:\n",
    "    if result:\n",
    "      correct += 1\n",
    "  \n",
    "  return (correct / predictions.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZE73sPOwTRWp"
   },
   "source": [
    "### The Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "4e37a7d950934142814cf90c380433a1",
      "2a71d790909e43fd8732382e36019371",
      "3d47fdec9b2248cc9f68602baef4f674",
      "54987a9a31994802ac7db2bcba1f8d0f",
      "1bf80c5be0a545c69e31f37ec7a00719",
      "348285d4d0784f27a384cb4dcec7b60a",
      "8d5ee593cd19475d80c8315caf097793",
      "c3e0cbe4cfcb4c1baa0f5f250da5a4d6"
     ]
    },
    "executionInfo": {
     "elapsed": 3426967,
     "status": "ok",
     "timestamp": 1621578014463,
     "user": {
      "displayName": "Danny Saad",
      "photoUrl": "",
      "userId": "08768728463630714575"
     },
     "user_tz": 240
    },
    "id": "GsbvPLEK-2d5",
    "outputId": "f36a041e-c485-45da-d55c-d3a1cfca4fbe"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:cgpg9g2d) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 52943<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/wandb/run-20210525_155046-cgpg9g2d/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/wandb/run-20210525_155046-cgpg9g2d/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">floral-lion-383</strong>: <a href=\"https://wandb.ai/witw/qualitative-analysis/runs/cgpg9g2d\" target=\"_blank\">https://wandb.ai/witw/qualitative-analysis/runs/cgpg9g2d</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "...Successfully finished last run (ID:cgpg9g2d). Initializing new run:<br/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.30 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.24<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">vibrant-rain-384</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/witw/qualitative-analysis\" target=\"_blank\">https://wandb.ai/witw/qualitative-analysis</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/witw/qualitative-analysis/runs/1wgktf15\" target=\"_blank\">https://wandb.ai/witw/qualitative-analysis/runs/1wgktf15</a><br/>\n",
       "                Run data is saved locally in <code>/home/wandb/run-20210525_155416-1wgktf15</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING SIZE: %i 100\n",
      "A shape pre encoder:  torch.Size([64, 1, 64, 64])\n",
      "A shape post encoder:  torch.Size([64, 512])\n",
      "A Size:  torch.Size([64, 512])\n",
      "AB Size:  torch.Size([128, 512])\n",
      "Squeezed AB Size:  torch.Size([128])\n",
      "A Size:  torch.Size([64, 512])\n",
      "AB Size:  torch.Size([128, 512])\n",
      "Squeezed AB Size:  torch.Size([128])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Using a target size (torch.Size([64])) that is different to the input size (torch.Size([128])) is deprecated. Please ensure they have the same size.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-215-059300507e89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m#test model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mbest_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_accuracy_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_accuracy_filtered\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_auroc_filtered\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_auroc_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetLowestError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     wandb.log({'Datasize': n_train,\n\u001b[1;32m     22\u001b[0m                \u001b[0;34m'Test_Loss'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbest_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-214-9533de8b1868>\u001b[0m in \u001b[0;36mgetLowestError\u001b[0;34m(model, train_loader, test_loader)\u001b[0m\n\u001b[1;32m     41\u001b[0m   \u001b[0mbest_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriteria\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriteria\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-213-6a944d1a550b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, criteria, device, loader, optimizer)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m### output is computing 128 probabilities instead of 64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriteria\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2517\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2518\u001b[0m         raise ValueError(\"Using a target size ({}) that is different to the input size ({}) is deprecated. \"\n\u001b[0;32m-> 2519\u001b[0;31m                          \"Please ensure they have the same size.\".format(target.size(), input.size()))\n\u001b[0m\u001b[1;32m   2520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2521\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([128])) is deprecated. Please ensure they have the same size."
     ]
    }
   ],
   "source": [
    "metacriteria_list = [metacriteria_attractive]                    \n",
    "trainingSize = np.logspace(2, 3.69897000434, 15).astype(np.int)\n",
    "\n",
    "\n",
    "for metacriteria in metacriteria_list:\n",
    "\n",
    "  run = wandb.init(project='qualitative-analysis', entity='witw', config = {\n",
    "        \"metacriteria\": metacriteria.__name__, \"pretrained\": True}, reinit = True)\n",
    "  wandb.run.name = \"CELEBA6-pretrained\" + metacriteria.__name__ \n",
    "\n",
    "  for n_train in trainingSize:\n",
    "    \n",
    "    print(\"TRAINING SIZE: %i\", n_train)\n",
    "    train_loader,test_loader = makeData(n_train, metacriteria)\n",
    "    \n",
    "    # make model\n",
    "    model = makeModel()\n",
    "    \n",
    "    #test model\n",
    "    best_test, best_epoch, best_accuracy_all, best_accuracy_filtered, best_auroc_filtered, best_auroc_all = getLowestError(model,train_loader,test_loader)\n",
    "    wandb.log({'Datasize': n_train,\n",
    "               'Test_Loss': best_test, \n",
    "               'AUROC_filtered': best_auroc_filtered, \n",
    "               'Accuracy_filtered': best_accuracy_filtered, \n",
    "               'Accuracy_all': best_accuracy_all, \n",
    "               'Epoch': best_epoch})\n",
    "    \n",
    "    print(\"Datasize: %i\", n_train)    \n",
    "    print(\"Best Test Loss: %0.3f. Best AUROC Filtered: %0.3f. Best AUROC All: %0.3f. Best Accuracy Filtered: %0.3f. Best Accuracy All: %0.3f. Best Epoch: %i\\n\" % (best_test, best_auroc_filtered, best_auroc_all, best_accuracy_filtered, best_accuracy_all, best_epoch))\n",
    "\n",
    "\n",
    "    if save_model:\n",
    "        torch.save(model.state_dict(), \"mnist_flat.pt\")\n",
    "\n",
    "  run.finish()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 514
    },
    "executionInfo": {
     "elapsed": 3083585,
     "status": "ok",
     "timestamp": 1619068282234,
     "user": {
      "displayName": "Danny Saad",
      "photoUrl": "",
      "userId": "08768728463630714575"
     },
     "user_tz": 240
    },
    "id": "6VBP3TGXCFSC",
    "outputId": "06e87a17-bd44-4b11-9b97-e89d6de11909"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ERPHXp06IaqH"
   },
   "source": [
    "# Visualize the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mTF9_SKlsR0i"
   },
   "outputs": [],
   "source": [
    "def get_encodings(model, device, loader):\n",
    "    model.eval()\n",
    "\n",
    "    outputs = []\n",
    "    targets = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (img, target) in enumerate(loader):\n",
    "          output = model(img.to(device))\n",
    "          outputs.append(output)\n",
    "          targets.append(target)\n",
    "\n",
    "    outputs = torch.cat(outputs)\n",
    "    targets = torch.cat(targets)\n",
    "\n",
    "    \n",
    "    return outputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "executionInfo": {
     "elapsed": 3084132,
     "status": "error",
     "timestamp": 1619068282812,
     "user": {
      "displayName": "Danny Saad",
      "photoUrl": "",
      "userId": "08768728463630714575"
     },
     "user_tz": 240
    },
    "id": "0kj5F6ATKzld",
    "outputId": "1fd52402-9c75-4e31-8abd-6aa66ba49adb"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-033404872640>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m image_loader_test = torch.utils.data.DataLoader(\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mimage_dataset_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     batch_size=batch_size_test, shuffle=True, **kwargs)\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'image_dataset_test' is not defined"
     ]
    }
   ],
   "source": [
    "encoder = model.encoder\n",
    "image_loader_test = torch.utils.data.DataLoader(\n",
    "    image_dataset_test,\n",
    "    batch_size=batch_size_test, shuffle=True, **kwargs)\n",
    "\n",
    "outputs, targets = get_encodings(encoder, device, image_loader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-_Etxs3bLA3J"
   },
   "outputs": [],
   "source": [
    "test_embeddings = outputs.cpu().numpy()\n",
    "targets = targets.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qgkxfkp_0UR_"
   },
   "outputs": [],
   "source": [
    "reducer = umap.UMAP()\n",
    "embedding = reducer.fit_transform(test_embeddings)\n",
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iA7UbJQr0jxd"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "sns.scatterplot(x=embedding[:,0], y=embedding[:,1], hue=targets.astype(str), hue_order=[str(i) for i in range(10)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cHUOtR9QzJRD"
   },
   "outputs": [],
   "source": [
    "pd.concat([pd.DataFrame(embedding), pd.DataFrame(targets)], axis = 1).to_csv(dataPath + \"least_common_multiple_UMAP.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wAbZ-9Yj0sxl"
   },
   "outputs": [],
   "source": [
    "def experiment_criteria(metacriteria, epochs=3, dataset_train=False, dataset_test=False, n_train=50000, n_test=5000, batch_size=64,batch_size_test=1000, UMAP=True):\n",
    "  #settings\n",
    "  lr=1\n",
    "  gamma=0.7\n",
    "  seed=42\n",
    "  no_cuda=False\n",
    "  use_cuda = not no_cuda and torch.cuda.is_available()\n",
    "  torch.manual_seed(seed)\n",
    "  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "  kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "\n",
    "  if not dataset_train and dataset_test:\n",
    "    #download training data if not included\n",
    "    dataset_train = datasets.MNIST('../data', train=True, download=True,\n",
    "                    transform=transforms.Compose([\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize((0.1307,), (0.3081,))\n",
    "                    ]))\n",
    "    # download testing data if not included\n",
    "    dataset_test = datasets.MNIST('../data', train=False, download=True,\n",
    "                    transform=transforms.Compose([\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize((0.1307,), (0.3081,))\n",
    "                    ]))\n",
    "  #train data\n",
    "  triplet_dataset_train = ImageTripletDataset(dataset_train, n_train, metacriteria)\n",
    "  train_loader = torch.utils.data.DataLoader(\n",
    "    triplet_dataset_train,\n",
    "    batch_size=batch_size, shuffle=True, **kwargs)\n",
    "  \n",
    "  #test data\n",
    "  triplet_dataset_test = ImageTripletDataset(dataset_test, n_test, metacriteria)\n",
    "  test_loader = torch.utils.data.DataLoader(\n",
    "    triplet_dataset_test,\n",
    "    batch_size=batch_size_test, shuffle=True, **kwargs)\n",
    "  \n",
    "  model = TripletNet().to(device)\n",
    "\n",
    "  optimizer = optim.Adadelta(model.parameters(), lr=lr)\n",
    "  scheduler = StepLR(optimizer, step_size=1, gamma=gamma)\n",
    "  criteria = nn.BCELoss()\n",
    "\n",
    "  for epoch in range(1, epochs + 1):\n",
    "    train_loss = train(model, criteria, device, train_loader, optimizer)\n",
    "    test_loss, outputs, targets = test(model, criteria, device, test_loader)\n",
    "    scheduler.step()\n",
    "\n",
    "    roc_auc = roc_auc_score(targets.cpu().numpy(), outputs.cpu().numpy())\n",
    "\n",
    "    print(\"Train loss: %0.3f. Test loss: %0.3f. AUROC: %0.3f\" % (train_loss, test_loss, roc_auc))\n",
    "\n",
    "  if UMAP:\n",
    "    encoder = model.encoder\n",
    "    image_loader_test = torch.utils.data.DataLoader(\n",
    "      dataset_test,\n",
    "      batch_size=batch_size_test, shuffle=True, **kwargs)\n",
    "\n",
    "    outputs, targets = get_encodings(encoder, device, image_loader_test)\n",
    "    \n",
    "    test_embeddings = outputs.cpu().numpy()\n",
    "    targets = targets.cpu().numpy() \n",
    "\n",
    "    reducer = umap.UMAP()\n",
    "    embedding = reducer.fit_transform(test_embeddings)\n",
    "    embedding.shape\n",
    "\n",
    "    plt.figure(figsize=(15,15))\n",
    "    sns.scatterplot(x=embedding[:,0], y=embedding[:,1], hue=targets.astype(str), hue_order=[str(i) for i in range(10)])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EuLcX5_Gt3U0"
   },
   "outputs": [],
   "source": [
    "experiment_criteria(metacriteria_least_common_multiple, epochs=6,dataset_train=image_dataset_train, dataset_test=image_dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cn3CrrJ07Zln"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CelebA Pretrained.ipynb",
   "provenance": [
    {
     "file_id": "1igpQstq1G3LvqRQqRZzzUhFIe2QgAgZI",
     "timestamp": 1614657191714
    },
    {
     "file_id": "137A9pWv-ypkT5E2qd4eCk8mgQJUwR8o1",
     "timestamp": 1604258134579
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1bf80c5be0a545c69e31f37ec7a00719": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2a71d790909e43fd8732382e36019371": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "348285d4d0784f27a384cb4dcec7b60a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3d47fdec9b2248cc9f68602baef4f674": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_348285d4d0784f27a384cb4dcec7b60a",
      "placeholder": "​",
      "style": "IPY_MODEL_1bf80c5be0a545c69e31f37ec7a00719",
      "value": " 0.01MB of 0.01MB uploaded (0.00MB deduped)\r"
     }
    },
    "4e37a7d950934142814cf90c380433a1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3d47fdec9b2248cc9f68602baef4f674",
       "IPY_MODEL_54987a9a31994802ac7db2bcba1f8d0f"
      ],
      "layout": "IPY_MODEL_2a71d790909e43fd8732382e36019371"
     }
    },
    "54987a9a31994802ac7db2bcba1f8d0f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c3e0cbe4cfcb4c1baa0f5f250da5a4d6",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8d5ee593cd19475d80c8315caf097793",
      "value": 1
     }
    },
    "8d5ee593cd19475d80c8315caf097793": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c3e0cbe4cfcb4c1baa0f5f250da5a4d6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
